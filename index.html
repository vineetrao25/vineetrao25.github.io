<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kemmannu Vineet Venkatesh Rao</title>
  
  <meta name="author" content="Kemmannu Vineet Venkatesh Rao">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="shortcut icon" href="https://umich.edu/favicon.ico"/>
  
<script type="text/javascript">
window.onload = function() {
  var image = document.getElementById("img");
  var num=1
  var total=3; 
  var imageWidth = 325; 
  var imageHeight = 400; 

  function updateImage() {
      num =num+1 
      if (num >total) {num=1} 
      image.src = "images/website_images/website_image"+ num +".jpg";
      
      image.style.width = imageWidth + "px";
      image.style.height = imageHeight + "px";
  }
  setInterval(updateImage, 10000);
}
</script>

</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name> K Vineet Venkatesh Rao</name>
              </p>
              <p> 
                Hi! I am an Master's graduate in Electrical Engineering and Computer Science from University of Michigan Ann Arbor, where I specialized in Signal, Image Processing, and Machine Learning. I am passionate about leveraging cutting-edge technology to create a meaningful and positive impact on the world.
              </p>
              <p>
                I obtained my undergraduate degree in Electronics and Communication Engineering, complemented by a minor in Computer Science, from PES University in 2020. My educational background, combined with my courses at the University of Michigan, have helped me develop a strong foundation in computer vision and machine learning.  
              </p>
              <p>
                If you have a similar passion for technology and innovation, I'd love to connect with you. Whether we explore potential research collaborations or simply engage in insightful conversations about the latest advancements in our field, feel free to get in touch with me. 
              </p>
              <p>
                Feel free to say hi at : <b><i>kemmannu<i> at umich dot edu</b>
              </p>
              <p style="text-align:center">
                <a href="data/Resume.pdf">CV</a> &nbsp/&nbsp
                <a href="https://twitter.com/vineetrao25">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/vineetrao25/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:40%">
              <a href="images/website_images/website_image1.jpg"><img width = "325" height = "400" alt="profile photo" src="images/website_images/website_image1.jpg" class="hoverZoomLink" id="img"> </a>
            </td>
            
          </tr> 
          <!-- Miniature logos row -->
          <tr style="padding:0px">
            <td colspan="1" style="text-align: center;">
              <img style="width: 30%; max-width: 130px; margin: 5px;" alt="Amazon Intern Logo" src="images/logos/amazon_intern_logo.png">
              <img style="width: 30%; max-width: 130px; margin: 5px;" width="300" alt="Amazon Lab126 Logo" src="images/logos/amazon_lab126_logo.png">
              <img style="width: 30%; max-width: 100px; margin: 5px;" alt="Umich Logo" src="images/logos/um_logo.png">
              <img style="width: 30%; max-width: 130px; margin: 5px;" alt="PES University Logo" src="images/logos/pesu_logo.png">
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                 I'm broadly interested in intersection Computer Vision, Deep Learning and NLP.  I am particularly interested in exploring self-supervised representational learning methods, utilizing multi-modal pretraining, especially language supervised pretraining, to address challenging vision tasks such as classification and detection. 
              </p> 
              <papertitle> Research Associate — Prof. Justin Johnson's AI Lab  &nbsp; (JAIL) </papertitle> 

                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  Aug. 2022 - Apr 2023  
              <p>
                Developing a novel training recipe for open vocabulary instance segmentation without the need for aligned data, inspired by the distinct ”what” and ”where” pathways observed in the human visual system (working towards
                submission for <b>ICLR 2024</b>)
              </p>
            </td>
          </tr>
        </tbody></table>  

        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Experience</heading>
            <br>
            <br>
            <papertitle>Applied Scientist Intern at Amazon Lab 126 </papertitle>  
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   May 2022 - Aug 2022
              <br>
              <em>Sunnyvale, CA </em>, Manager:<a href="https://www.linkedin.com/in/prasadshamain/">Prasad Shamain</a>               
              <br>
              <p>
                Conducted research, collaboration, and implementation of a privacy-focused Deep Learning solution for a complex problem involving radar inputs for a specific Amazon product
              </p>  
              <p>
                Designed and implemented a Computer Vision Ground Truth system, Data-Collection pipeline, and synchronization, while exploring different deep learning methods for a Radar Based System, considering resource constraints and providing key insights for decision-making
              </p>

          </td> 
          </tr>
        </tbody></table> 

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Projects</heading>
            <p>
               Here are some of the projects that I have been working on :-)
            </p>
          </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/project_images/ml_project.png" alt="SSL Object Detector" width="200" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <papertitle>Self-Supervised Object Detection with Multimodal Image Captioning</papertitle>
              <br>
              <em>EECS 545 Course Project </em>, Supervisor :<a href="https://web.eecs.umich.edu/~honglak/">Honglak Lee</a>
              <br>
              <a href="data/eecs545_report.pdf">Report</a>
              /
              <a href="data/eecs545_poster.pdf">Poster</a>
              <br>
              <p>Our novel proposed method drastically reduces the need for human supervision required for training an Object Detector. We leveraged a language supervised pre-trained bi-directional captioning model (VirTex-v2) that outputs a set of diverse captions given an image.</p> 
              <p>Further, we performed zero-shot transfer using prompt engineering and then filtered the results using noun and target extraction modules to obtain a set of "words of interest" from the generated captions. With these "words of interest," we utilized Gradient Class Activation Maps to localize an object within an image and draw a bounding box around it. This created pseudo labels and pseudo bounding boxes that acted as supervision to train the Object Detector.</p>
              <p>Finally, we trained a Fully Convolution Single Stage Object Detector (FCOS) using a new small modified version of GioU loss to account for the noise in the generated pseudo bounding boxes. Our techniques were verified on the Pascal-VOC dataset.</p>
            </td>
          </tr>
        </tbody></table>
        <br>
        <br>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/project_images/ml_project.png" alt="SSL Object Detector" width="200" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <papertitle>MC - VQA Using Customized Prompts </papertitle>
              <br>
              <em>EECS 595 Course Project </em>, Supervisor :<a href="https://web.eecs.umich.edu/~chaijy/">Prof. Joyce Chai</a>
              <br>
              <a href="data/eecs595_report.pdf">Report</a>
              <br>
              <p> In this course project, we developed a novel pipeline to perform zero-shot Visual Question Answering (VQA) by combining large pre-trained models, such as CLIP and the T-5 transformer. </p>  
              <p> Our approach involved converting questions into declarative statements and leveraging them alongside images and multiple-choice answers to perform zero-shot VQA using CLIP.</p>
              <p> Our pipeline achieved an impressive total accuracy of 49.5%, which is comparable to the state-of-the-art (SOTA) performance in zero-shot VQA and outperformed the previous SOTA in this task. This success demonstrates the effectiveness of our customized prompts and the potential of large pre-trained models for zero-shot VQA.</p>
            </td>
          </tr>
        </tbody></table>
        <br>
        <br> 



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/project_images/ml_project.png" alt="SSL Object Detector" width="200" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <papertitle>ToddlerNet: Exploring Data Diversity versus View Diversity</papertitle>
              <br>
              <em>EECS 598 : Action and Perception Course Project </em>, Supervisor :<a href="https://web.eecs.umich.edu/~stellayu/">Dr. Stella Yu </a>
              <br>
              <a href="data/eecs598_008_report.pdf">Report</a>
              <br>
              <p> This course project focused on the exploration of data diversity versus view diversity in representation learning using the novel approach named ToddlerNet. Inspired by physological concepts by Dr. Linda Smith, we aimed to leverage insights from infant perception research to enhance deep learning models. </p>  
              <p>Our investigation primarily centered on understanding the significance of data statistics resembling infants' everyday environment in improving object recognition across different contexts and viewpoints. ToddlerNet demonstrated promising results by training algorithms on data exhibiting infants' perceptual experiences, leading to a more robust recognition of objects under various viewing conditions.</p> 
              <p>Additionally, we explored optimal learning objectives and discovered that pre-training with a supervised contrastive learning objective on view-diverse data yielded superior results in ToddlerNet's performance. This finding showcases the potential of leveraging view diversity in representation learning for enhanced object recognition capabilities.
          
            </td>
          </tr>
        </tbody></table>
        <br>
        <br>




        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/project_images/dlcv_project.png" alt="Fine-grained Food Classification" width="200" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <papertitle>Language Supervised Vision Pre-Training for Fine-grained Food Classification </papertitle>
              
              <br>
              <em>EECS 598 Course Mini-Project </em>, Supervisor :<a href="https://web.eecs.umich.edu/~justincj/">Justin Johnson</a>
              <br>
              <a href="data/eecs598_report.pdf">Report</a>
              <br>
              <p>For this research project, we compiled a unique and curated dataset, referred to as "SubRedCaps," consisting of image-caption pairs extracted from sub-reddits specifically related to food. The dataset spans the time period from April 2020 to April 2022.</p>
              <p> To enable our study of food image captioning, we pre-trained a bi-directional image captioning model using RegNeX-800MF as the feature extractor for the images. Additionally, we utilized a two-layer encoder-decoder transformer architecture to learn rich semantic representations from the captions, leveraging our subsampled SubRedCaps dataset.</p>
              <p>Despite working with limited hardware resources, our best model achieved a remarkable 20% zero-shot accuracy on the test-set of the challenging Food-101 dataset. This noteworthy result demonstrates the efficacy of our approach in generating accurate captions for food-related images.</p>
            </td>
          </tr>
        </tbody></table>
        <br>
        <br>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/project_images/ml_vlsi.png" alt="Fine-grained Food Classification" width="200" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <papertitle>Quantized Winograd Convolution based accerlator for Convolutional Neural Networks  </papertitle>
              <br>
              <em>EECS 598 Course Project </em>, Supervisor :<a href="https://kim.engin.umich.edu/">Hun-Seok Kim</a>
              <br>
              <a href="data/eecs598_001_report.pdf">Report</a>
              <br>
              <p>We designed and developed an 8-bit quantized flexible Winograd-based convolutional engine in Verilog to achieve decreased inference time and model size for Convolutional Neural Networks (CNNs). To validate the design, we conducted simulations of the entire inference cycle of a CNN using MATLAB.</p> 
              <p>Our investigation primarily focused on exploring and implementing various quantization techniques widely used in tinyML (Tiny Machine Learning) to effectively reduce model complexity while preserving accuracy.</p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>



<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr style="padding:0px">
    <td style="padding:0px">

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody>
      </table>
      
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/logos/um_logo.png" alt="cs188" width="128">
          </td>
          <td width="75%" valign="center">
            Graduate Student Instructor 
            <br>
            <br>
            <a href="https://ece.engin.umich.edu/academics/course-information/course-descriptions/eecs-452/">EECS 452:  Digital Signal Processing Laboratory</a> (Fall 2022), <a href="https://www.birds.eecs.umich.edu/"> Prof. Shai Revzen </a>
            <br>
            <br>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/logos/um_logo.png" alt="cs188" width="128">
          </td>
          <td width="75%" valign="center">
            Graduate Student Instructor 
            <br>
            <br>
            <a href="https://ece.engin.umich.edu/academics/course-information/course-descriptions/eecs-452/">EECS 452:  Digital Signal Processing Laboratory</a> (Winter 2023), <a href="https://arsarabi.github.io/"> Prof. Armin Sarabi </a>
            <br>
            <br>
          </td>
        </tr>
      </tbody></table>
    </td> 
  </tr> 
</table>  

  Website style borrowed from  <a href="https://jonbarron.info/">here</a>.
</body> 
</html>
